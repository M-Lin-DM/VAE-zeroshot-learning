{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import csv\n",
    "print(tf.__version__)\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image processing methods\n",
    "def process_image(imgtensor):\n",
    "    imgtensor *= (1/255) #\n",
    "    return imgtensor\n",
    "\n",
    "def deprocess_image(imgtensor):\n",
    "    imgtensor *= 255\n",
    "    imgtensor = np.clip(imgtensor, 0, 255)\n",
    "    return imgtensor\n",
    "\n",
    "train_dir = 'D:/Datasets/VAE_zeroshot/data_punctured_1p25-1_R0p3_RESIZE/train' #your images must be inside a subfolder in the last folder of datadir. if using multiple classes, put each class in one subfolder of the train_data folder. also put validation and test sets each in their own subfolder at the same level as the training folder\n",
    "validation_dir = 'D:/Datasets/VAE_zeroshot/data_punctured_1p25-1_R0p3_RESIZE/test' #your images must be inside a subfolder in the last folder of datadir. if using multiple classes, put each class in one subfolder of the train_data folder. also put validation and test sets each in their own subfolder at the same level as the training folder\n",
    "hole_dir = 'D:/Datasets/VAE_zeroshot/data_hole_1p25-1_R0p3_RESIZE'\n",
    "# train_dataframe_dir = 'D:/Datasets/VAE_zeroshot/data_punctured_1p25-1_R0p3_RESIZE/train/category1' #your images must be inside a subfolder in the last folder of datadir. if using multiple classes, put each class in one subfolder of the train_data folder. also put validation and test sets each in their own subfolder at the same level as the training folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(538, shape=(), dtype=int64)\n",
      "Found 4299 files belonging to 1 classes.\n",
      "tf.Tensor(538, shape=(), dtype=int64)\n",
      "tf.Tensor(538, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#set up training tf.data.Dataset so that sample weights can be given as model input\n",
    "\n",
    "#load sample weight data\n",
    "file_CSV = open('data_punctured_weights_train.csv')\n",
    "data_CSV = csv.reader(file_CSV)\n",
    "list_CSV = list(data_CSV)\n",
    "# print(list_CSV[0])\n",
    "res = [float(i) for i in list_CSV[0]] # convert list of str to list of float\n",
    "res = np.array(res)\n",
    "# print(res)\n",
    "\n",
    "#make dataset that will be zipped to the images dataset to indicate sample weights\n",
    "weights_dataset = tf.data.Dataset.from_tensor_slices(res)\n",
    "weights_dataset = weights_dataset.batch(8, drop_remainder=False)\n",
    "print(weights_dataset.cardinality())\n",
    "\n",
    "#create images dataset\n",
    "#set shuffle to False since this dataset must match up with the weights dataset (which isnt shuffled)\n",
    "img_dataset = image_dataset_from_directory(\n",
    "    train_dir, shuffle = False, image_size=(256, 256), batch_size=8, color_mode='grayscale', validation_split=None, label_mode=None)\n",
    "\n",
    "#rescale pixel values\n",
    "img_dataset = img_dataset.map(process_image)\n",
    "print(img_dataset.cardinality())\n",
    "\n",
    "#Proper format for dataset output!: ((input image, sample weight), target output))\n",
    "#the zipping of these two datasets will create a dataset that generates tuples: (image array (batch), sample weight (batch))\n",
    "dataset_with_weights_tuple1 = tf.data.Dataset.zip((img_dataset, weights_dataset))\n",
    "dataset_with_weights_train = tf.data.Dataset.zip((dataset_with_weights_tuple1, img_dataset))\n",
    "\n",
    "print(dataset_with_weights_train.cardinality())\n",
    "del weights_dataset, img_dataset, dataset_with_weights_tuple1, res, file_CSV, data_CSV, list_CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(27, shape=(), dtype=int64)\n",
      "Found 211 files belonging to 1 classes.\n",
      "tf.Tensor(27, shape=(), dtype=int64)\n",
      "tf.Tensor(27, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#set up validation dataset\n",
    "\n",
    "file_CSV = open('data_punctured_weights_validation.csv')\n",
    "data_CSV = csv.reader(file_CSV)\n",
    "list_CSV = list(data_CSV)\n",
    "# print(list_CSV[0])\n",
    "res = [float(i) for i in list_CSV[0]] # convert list of str to list of float\n",
    "res = np.array(res)\n",
    "# print(res)\n",
    "\n",
    "#make dataset that will be zipped to the images dataset to indicate sample weights\n",
    "weights_dataset = tf.data.Dataset.from_tensor_slices(res)\n",
    "weights_dataset = weights_dataset.batch(8, drop_remainder=False)\n",
    "print(weights_dataset.cardinality())\n",
    "\n",
    "#create images dataset\n",
    "img_dataset = image_dataset_from_directory(\n",
    "    validation_dir, shuffle = False, image_size=(256, 256), batch_size=8, color_mode='grayscale', validation_split=None, label_mode=None)\n",
    "\n",
    "#rescale pixel values\n",
    "img_dataset = img_dataset.map(process_image)\n",
    "print(img_dataset.cardinality())\n",
    "\n",
    "#Proper format for dataset output!: ((input image, sample weight), target output))\n",
    "# dataset_with_weights = img_dataset.map(lambda x: ((x, 1), 0))\n",
    "#the zipping of these two datasets will create a dataset that generates tuples: (image array (batch), sample weight (batch))\n",
    "dataset_with_weights_tuple1 = tf.data.Dataset.zip((img_dataset, weights_dataset))\n",
    "dataset_with_weights_validation = tf.data.Dataset.zip((dataset_with_weights_tuple1, img_dataset))\n",
    "\n",
    "print(dataset_with_weights_validation.cardinality())\n",
    "del weights_dataset, img_dataset, dataset_with_weights_tuple1, res, file_CSV, data_CSV, list_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(62, shape=(), dtype=int64)\n",
      "Found 490 files belonging to 1 classes.\n",
      "tf.Tensor(62, shape=(), dtype=int64)\n",
      "tf.Tensor(62, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#set up \"hole\" dataset\n",
    "\n",
    "file_CSV = open('data_hole_weights.csv')\n",
    "data_CSV = csv.reader(file_CSV)\n",
    "list_CSV = list(data_CSV)\n",
    "# print(list_CSV[0])\n",
    "res = [float(i) for i in list_CSV[0]] # convert list of str to list of float\n",
    "res = np.array(res)\n",
    "# print(res)\n",
    "\n",
    "#make dataset that will be zipped to the images dataset to indicate sample weights\n",
    "weights_dataset = tf.data.Dataset.from_tensor_slices(res)\n",
    "weights_dataset = weights_dataset.batch(8, drop_remainder=False)\n",
    "print(weights_dataset.cardinality())\n",
    "\n",
    "#create images dataset\n",
    "img_dataset = image_dataset_from_directory(\n",
    "    hole_dir, shuffle = False, image_size=(256, 256), batch_size=8, color_mode='grayscale', validation_split=None, label_mode=None)\n",
    "\n",
    "#rescale pixel values\n",
    "img_dataset = img_dataset.map(process_image)\n",
    "print(img_dataset.cardinality())\n",
    "\n",
    "#Proper format for dataset output!: ((input image, sample weight), target output))\n",
    "# dataset_with_weights = img_dataset.map(lambda x: ((x, 1), 0))\n",
    "#the zipping of these two datasets will create a dataset that generates tuples: (image array (batch), sample weight (batch))\n",
    "\n",
    "dataset_with_weights_tuple1 = tf.data.Dataset.zip((img_dataset, weights_dataset))\n",
    "dataset_with_weights_hole = tf.data.Dataset.zip((dataset_with_weights_tuple1, img_dataset))\n",
    "\n",
    "print(dataset_with_weights_hole.cardinality())\n",
    "del weights_dataset, img_dataset, dataset_with_weights_tuple1, res, file_CSV, data_CSV, list_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAE\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 254, 254, 64) 640         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 254, 254, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 127, 127, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 125, 125, 64) 36928       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 125, 125, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 63, 63, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 61, 61, 128)  73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 61, 61, 128)  512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 31, 31, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 29, 29, 128)  147584      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 29, 29, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 15, 15, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 28800)        0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            86403       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sample_latent_vector (Sample_la (None, 3)            0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16384)        49152       sample_latent_vector[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 32, 32, 16)   0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoderupsample1 (UpSampling2D) (None, 64, 64, 16)   0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 64, 64, 64)   9280        decoderupsample1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 128, 128, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 128, 128, 64) 36928       up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128, 128, 64) 256         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 256, 256, 64) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 256, 256, 128 73856       up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 256, 256, 128 512         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 256, 256, 256 33024       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 256, 256, 256 1024        conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 1)  257         batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sample_weights (InputLayer)     [(None, 1, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add_vae__loss (Add_VAE_Loss)    (None, 256, 256, 1)  0           encoder_input[0][0]              \n",
      "                                                                 dense[0][0]                      \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "                                                                 sample_weights[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 551,492\n",
      "Trainable params: 549,700\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build encoder model\n",
    "latent_dim = 3 #dimensionality of latent space\n",
    "\n",
    "encoder_input = Input(shape=(256, 256, 1), name='encoder_input')\n",
    "\n",
    "x = layers.Conv2D(64, 3, strides=(1, 1), activation='relu', padding='valid', kernel_initializer='RandomNormal',  bias_initializer='zeros')(encoder_input)\n",
    "\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(64, 3, strides=(1, 1), activation='relu', padding='valid', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(128, 3, strides=(1, 1), activation='relu', padding='valid', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(128, 3, strides=(1, 1), activation='relu', padding='valid', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Flatten(data_format='channels_last')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, activation='linear', name='z_mean', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "\n",
    "## subclass keras layer to sample the latent vector \n",
    "class Sample_latent_vector(keras.layers.Layer):\n",
    "    def sample(self, inputs):\n",
    "        sigma = 0.1\n",
    "        z_mean = inputs\n",
    "        epsilon = K.random_normal(shape=K.shape(z_mean))\n",
    "        #output is batch of latent vectors\n",
    "        return z_mean + epsilon*sigma\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.sample(inputs)\n",
    "    \n",
    "z = Sample_latent_vector()(z_mean)\n",
    "\n",
    "# Build Decoder network\n",
    "\n",
    "x = layers.Dense(16384, activation='relu', use_bias=False, kernel_initializer='RandomNormal')(z)\n",
    "x = layers.Reshape((32, 32, 16))(x)\n",
    "x = layers.UpSampling2D(2, name='decoderupsample1')(x)\n",
    "x = layers.Conv2DTranspose(64, 3, strides=(1, 1), activation='relu', padding='same', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "x = layers.BatchNormalization(axis=-1)(x)\n",
    "x = layers.UpSampling2D(2)(x)\n",
    "x = layers.Conv2DTranspose(64, 3, strides=(1, 1), activation='relu', padding='same', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "x = layers.BatchNormalization(axis=-1)(x)\n",
    "x = layers.UpSampling2D(2)(x)\n",
    "x = layers.Conv2DTranspose(128, 3, strides=(1, 1), activation='relu', padding='same', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "x = layers.BatchNormalization(axis=-1)(x)\n",
    "x = layers.Conv2DTranspose(256, 1, strides=(1, 1), activation='relu', padding='same', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "x = layers.BatchNormalization(axis=-1)(x)\n",
    "z_decoded = layers.Conv2DTranspose(1, 1, strides=(1, 1), activation='sigmoid', padding='same', kernel_initializer='RandomNormal',  bias_initializer='zeros')(x)\n",
    "\n",
    "\n",
    "# make custom loss function by subclassing Layer and using add_loss\n",
    "class Add_VAE_Loss(keras.layers.Layer):\n",
    "    \n",
    "    def compute_VAE_loss(self, inputs):\n",
    "        sigma = 0.1\n",
    "        input_image, z_mean, output_image, sample_weights = inputs\n",
    "        KL_loss = K.mean(K.square(z_mean) + K.exp(K.log(K.square(sigma))) - K.log(K.square(sigma)) - 1, axis=1)\n",
    "        recon_loss = keras.metrics.binary_crossentropy(K.flatten(input_image), K.flatten(output_image))\n",
    "        return K.mean((8e-3 * KL_loss + 1*recon_loss)*sample_weights*2, axis=1) #sample weight is provided as an output from the dataset (generator)\n",
    "        \n",
    "        \n",
    "    def call(self, layer_inputs):\n",
    "        if not isinstance(layer_inputs, list):\n",
    "            ValueError('Input must be list of [input_image, z_mean, output_image]')\n",
    "        loss = self.compute_VAE_loss(layer_inputs)\n",
    "        self.add_loss(loss)\n",
    "        return layer_inputs[2]\n",
    "    \n",
    "#Make VAE model \n",
    "outputs = Add_VAE_Loss()([encoder_input, z_mean, z_decoded, sample_weights])\n",
    "VAE = Model([encoder_input, sample_weights], outputs, name='VAE')\n",
    "VAE.summary()\n",
    "\n",
    "VAE.compile(loss=None,\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics = ['binary_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0524s). Check your callbacks.\n",
      "WARNING:tensorflow:From C:\\Users\\MrLin\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\MrLin\\anaconda3\\envs\\tf_3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.5095 - binary_crossentropy: 0.5120 - val_loss: 0.1765 - val_binary_crossentropy: 0.1591\n",
      "Epoch 2/10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.1865 - binary_crossentropy: 0.2947 - val_loss: 0.1444 - val_binary_crossentropy: 0.3252\n",
      "Epoch 3/10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.1265 - binary_crossentropy: 0.2533 - val_loss: 0.1134 - val_binary_crossentropy: 0.2067\n",
      "Epoch 4/10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.1038 - binary_crossentropy: 0.2046 - val_loss: 0.1009 - val_binary_crossentropy: 0.1933\n",
      "Epoch 5/10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.0946 - binary_crossentropy: 0.1742 - val_loss: 0.0964 - val_binary_crossentropy: 0.1503\n",
      "Epoch 6/10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.0899 - binary_crossentropy: 0.1444 - val_loss: 0.0938 - val_binary_crossentropy: 0.2051\n",
      "Epoch 7/10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.0870 - binary_crossentropy: 0.1412 - val_loss: 0.0878 - val_binary_crossentropy: 0.1255\n",
      "Epoch 8/10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.0846 - binary_crossentropy: 0.1109 - val_loss: 0.0874 - val_binary_crossentropy: 0.1280\n",
      "Epoch 9/10\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D\\assets\n",
      "538/538 - 50s - loss: 0.0847 - binary_crossentropy: 0.1280 - val_loss: 0.0866 - val_binary_crossentropy: 0.1058\n",
      "Epoch 10/10\n",
      "538/538 - 44s - loss: 0.0837 - binary_crossentropy: 0.1087 - val_loss: 0.0866 - val_binary_crossentropy: 0.1069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2497a4e0cc8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = r'C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance_Z3D'\n",
    "#Make checkpoint callback \n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "VAE.fit(dataset_with_weights_train,\n",
    "        shuffle=True,\n",
    "        steps_per_epoch=538,#steps_per_epoch needs to equal exactly the number of batches in the Dataset generator\n",
    "        epochs=10,\n",
    "        verbose = 2,\n",
    "        validation_data=dataset_with_weights_validation,\n",
    "        validation_steps=27,#=#batches in validation dataset\n",
    "        callbacks=[model_checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE = keras.models.load_model(r\"C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighted_const_variance\")\n",
    "\n",
    "# VAE.save_weights('VAE_weighteddata_constvar')\n",
    "# VAE.save('VAE_model_df')\n",
    "#Load in weights to VAE. Then copy its latter layers to a new decoder model\n",
    "# weights_path = r'C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\VAE_weighteddata_constvar'\n",
    "# VAE.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICT images \n",
    "\n",
    "#visualize image reconstructions\n",
    "# train_sample = dataset_with_weights_train.take(24)\n",
    "validation_sample = dataset_with_weights_validation.take(24)\n",
    "# hole_sample = dataset_with_weights_hole.take(24)\n",
    "\n",
    "xhat = VAE.predict(validation_sample, steps=3)\n",
    "# xhat = VAE_2.predict(train_sample, steps=3)\n",
    "xhat = deprocess_image(xhat)\n",
    "xhat = xhat.astype(np.uint8)\n",
    "xhat.shape\n",
    "for j in range(20):\n",
    "    pil_im = Image.fromarray(xhat[j,:,:,0],mode='L') # convert each slice of predicted batch to a PIL image object  \n",
    "    pil_im.save(r'C:\\Users\\MrLin\\Documents\\Experiments\\VAE_zeroshot\\Results\\pred_' + f'{j+4300:02}' + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "latent_vector (InputLayer)   [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16384)             524288    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "decoderupsample1 (UpSampling (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 64)        9280      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 256, 256, 128)     73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256, 256, 128)     512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 256, 256, 256)     33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256, 256, 256)     1024      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 256, 256, 1)       257       \n",
      "=================================================================\n",
      "Total params: 679,681\n",
      "Trainable params: 678,657\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# make decoder model by copying VAE layers to a new model\n",
    "VAE.layers[16]\n",
    "decoder_input = Input(shape = (latent_dim), name='latent_vector')\n",
    "x = decoder_input\n",
    "for layer in VAE.layers[16:-2]:\n",
    "#     print(layer.name)\n",
    "    x = layer(x)\n",
    "    \n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d\n",
      "batch_normalization\n",
      "max_pooling2d\n",
      "conv2d_1\n",
      "batch_normalization_1\n",
      "max_pooling2d_1\n",
      "conv2d_2\n",
      "batch_normalization_2\n",
      "max_pooling2d_2\n",
      "conv2d_3\n",
      "batch_normalization_3\n",
      "max_pooling2d_3\n",
      "flatten\n",
      "dense\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "detached_encoder_input (Inpu [(None, 256, 256, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 254, 254, 64)      640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 254, 254, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 125, 125, 64)      36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 125, 125, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 61, 61, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 61, 61, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 29, 29, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 29, 29, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                921632    \n",
      "=================================================================\n",
      "Total params: 1,182,176\n",
      "Trainable params: 1,181,408\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#make encoder that skips the sampling layer and just returns z_mean\n",
    "\n",
    "VAE.layers[14]\n",
    "detached_encoder_input = Input(shape=(256, 256, 1), name='detached_encoder_input')\n",
    "x = detached_encoder_input #\n",
    "for layer in VAE.layers[1:15]:#we must skip the VAE input layer (for loop starts at layer 1 instead of 0) as it causes problems to use the same input layer in two separate models\n",
    "    x = layer(x)\n",
    "    \n",
    "encoder = Model(detached_encoder_input, x, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"VAE_sampless\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "VAE_2_input (InputLayer)     [(None, 256, 256, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         (None, 32)                1182176   \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 256, 256, 1)       679681    \n",
      "=================================================================\n",
      "Total params: 1,861,857\n",
      "Trainable params: 1,860,065\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create Model for inference which skips the sample_latent_vector layer and just uses z_mean as the latent vector\n",
    "VAE_2_input = Input(shape=(256, 256, 1), name='VAE_2_input')\n",
    "VAE_2 = Model(VAE_2_input, decoder(encoder(VAE_2_input)), name=\"VAE_sampless\")\n",
    "VAE_2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
